---
title: 华为网卡项目1
date: 2021-05-13 16:08:50
tags: 华为项目
typora-root-url: ..
---

## NVMe-oF概述

NVMe-oF全称是：Non-Volatile Memory Express over Fabrics。中文的意思是：基于Fabrics的非易失性存储标准。我将Fabric理解为网络架构，RDMA Fabric就有Infiniband，iWARP，RoCEv1，RoCEv2等多种传输协议。

### NVMe-oF和NVMe的关系

NVMe是什么：NVMe传输是一种抽象协议层，旨在提供可靠的NVMe命令和数据传输，是有关访问非易失性储存（通常是 SSD 磁盘）的接口标准。本质是上建立了多个计算机与存储设备的通路，提高搬运数据的速度。

![image-20211230163244922](D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230163244922.png)

当前数据中心中的存储网络架构通常使用的是存储区域网络 (Storage Area Network, SAN) ，SAN 是一种基于块的存储，利用高速架构将服务器与其逻辑磁盘单元 (Logical Disk Unit, LUN) 相连。当前的SAN通常使用采用FC（Fibre Channel）光纤通道或者Ethernet进行远距离传输。其中SAN网络环境中，因采用存储设备类型的不同又可以分为FC-SAN(采用光纤通道存储产品)和IP-SAN(采用ISCSI存储设备)。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230170745869.png" alt="image-20211230170745869" style="zoom:50%;" />

**NVMe-oF的目的就是利用现有成熟的Fabric来跑NVMe，实现NVMe标准在PCIe总线上的扩展也就是把本地高速访问的优势暴露给远端应用。**NVMe-oF可以实现ISCISI的功能，甚至可以更好。在长时间内，NVMe-oF和iSCSI还是长期并存的局面。iSCSI目前已经非常成熟，而NVMe-oF则刚刚开始发展，需要不断地完善，并且借鉴iSCSI协议的一些功能，以支持更多的功能。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230170818645.png" alt="image-20211230170818645" style="zoom:50%;" />

### NVMe-oF

2016年发布的 NVMe-oF 1.0 规范描述了两种Fabric，光纤通道(Fibre Channel)和远程直接内存访问 (RDMA) 。下图是NVME和各种Fabric的关系。可以看到NVMe-oF是在Fabric之上的。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230171628666.png" alt="image-20211230171628666" style="zoom:50%;" />

上图可以更好的展示为，左侧三个灰色框架代表的是NVMe在一个机器内的操作，右边是通过NVMe-oF把NVMe扩展到数据中心中。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230172937194.png" alt="image-20211230172937194" style="zoom:50%;" />

<img src="D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230193645449.png" alt="image-20211230193645449" style="zoom:67%;" />

NVMe-oF的协议栈如下

<img src="D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230195604296.png" alt="image-20211230195604296" style="zoom:50%;" />

<img src="D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230201734587.png" alt="image-20211230201734587" style="zoom:50%;" />

NVMeoF协议栈访问流程：NVMe控制器端CSCCX，转换成NVMe命令，写完数据后，数据怎么返回去的。

![image-20220725111730264](D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20220725111730264.png)

### NVMe Transport Mapping（NVME传输映射）

**NVMe over Fabrics 需要底层 NVMe 传输来提供可靠的 NVMe 命令和数据传输。**

在本地 NVMe 实现中，NVMe 命令和响应通过 PCIe 接口映射到主机中的共享内存。然而，fabric 是建立在发送和接收消息的概念之上的，在端之间没有共享内存。NVMe fabric消息传输旨在通过使用包含一个或多个 NVMe 命令或响应的“capsules”将 NVMe 命令和响应封装到基于消息的系统中。 capsules或capsules与数据的组合独立于特定的fabric，并通过所需fabric发送和接收。就是说**NVMe和Fabric之间没有指定的关系，是松耦合的**。 **NVMe 传输是独立于任何物理互连属性的抽象协议层。**

 在NVMe over Fabrics中，整个 NVMe 多队列模型得到维护，使用普通的 NVMe 提交队列(submission queues)和完成队列(completion queues)，但封装在基于消息的传输上。 NVMe I/O 队列对(submission and completion)是为多核 CPU 设计的，这种低延迟的高效设计在 NVMe over Fabrics 中得到维护。

 下图显示了 NVMe 传输的分类和示例。 NVMe 传输包含内存模型、消息模型或两者的组合。 内存模型是一种通过执行显式内存读写操作在结构节点之间传输命令、响应和数据的模型，而消息模型是一种仅在结构节点之间发送包含命令capsules、响应capsules和数据的消息。 消息/内存模型使用消息和显式内存读写操作的组合在结构节点之间传输命令包、响应包和数据。 数据可以选择性地包含在命令包和响应包中。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\Bruce-sun-data.github.io\images\华为网卡项目\image-20211230192640428.png" alt="image-20211230192640428" style="zoom:40%;" />

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\Bruce-sun-data.github.io\images\华为网卡项目\image-20211230185100624.png" alt="image-20211230185100624" style="zoom:50%;" />

```
上面这两个图一个来自OverView（第二个图），一个来自specification(第一个图)，感觉有点冲突。
```

在向 NVMe over Fabrics 设备发送复杂消息时，capsules允许将多条小消息作为一条消息发送，从而提高传输效率并减少延迟。 capsules是一个提交队列条目(submission queue entry)或一个完成队列条目(completion queue entry)，结合了一定数量的数据、元数据或 Scatter-Gather Lists (SGLs)。 元素的内容与本地 NVMe 协议相同，但capsules是一种将它们打包在一起以提高效率的方式。下图是capsules的具体架构。

![image-20211230190840475](D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20211230190840475.png)

### NVMe

NVMe是什么：NVMe是一种Host与SSD之间通讯的协议，它在协议栈中隶属高层。

![img](https://img-blog.csdn.net/20180417100139590)

NVMe制定了Host与SSD之间通讯的命令，以及命令如何执行的。

NVMe有两种命令，一种叫Admin Command，用以Host管理和控制SSD；另外一种就是I/O Command，用以Host和SSD之间数据的传输。

NVMe有三种队列：Submission Queue （SQ），Completion Queue（CQ）和Doorbell Register （DB）。 SQ和CQ位于Host的内存中，DB则位于SSD的控制器内部。如下图所示：

![img](https://img-blog.csdn.net/20180417100922685)

SQ位于Host内存中，Host要发送命令时，先把准备好的命令放在SQ中，然后通知SSD来取；CQ也是位于Host内存中，一个命令执行完成，成功或失败，SSD总会往CQ中写入命令完成状态。DB则是在Host发送命令时，不是直接往SSD中发送命令的，而是把命令准备好放在自己的内存中，Host就是通过写SSD端的DB寄存器来告知SSD命令已经处理完毕，可以前来检阅。流程如下图所示：

![img](https://img-blog.csdn.net/20180417101200769)

1. Host写命令到SQ，此时的命令在host的内存中；
2. Host写DB，通知NVMe Controller取命令。通过更新在Controller内部的寄存器SQ Tail Doorbell来完成。
3. Controller收到通知，于是从SQ中取命令。取走命令之后，需要在Controller内部的SQ Head Pointer寄存器中更新Head所在的位置。NVMe没有规定Command存入队列的执行顺序，Controller可以一次取出多个Command进行批量处理。
4. Controller执行命令。执行Read/Wirte Command时，这个过程也会与Host Memory进行数据传递。
5. 命令执行完成，Controller往CQ中写指令执行结果。此时，Controller需要更新内部的CQ Tail Pointer寄存器。
6. Controller发短信通知Host指令完成。Controller通过发送一个中断信息告知Host。
7. 收到短信，Host处理CQ，查看命令完成状态；
8. Host处理完CQ中的命令需要告知controller。Host更新Controller内部的CQ Head Doorbell。

四个寄存器全部放在Controller内存中。也就是说Controller知道这SQ Tail/Head和CQ Tail/Head的全部信息。

而Host仅仅知道自己更新的两个信息SQ Tail和CQ Head。Controller把SQ Head和CQ Tail的信息写入Completion报文中，通知给host。

NVMe 访存路径

![image-20220725111306065](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20220725111306065.png)

###  NVMe over ROCE

NVMe over RoCE 调用关系如下图所示，内核 nvme_rdma 模块相当于胶水层，连接 rdma stack 和 nvme core接口，即 NVMe 队列接口可以对接 RDMA 队列接口，进而调用下层 rdma stack 中 verbs 传输接口。

<img src="https://pic4.zhimg.com/v2-adaad1b3a14e2fa115fe0472c8b0955f_r.jpg" alt="preview" style="zoom:50%;" />

#### NVMeoF队列结构

NVMeoF将NVMe队列映射到RDMA队列上。

![image-20220725112852892](D:\Sun1999\MyHexoBlogs\myblogs\Bruce-sun-data.github.io\source\_posts\images\华为网卡项目\image-20220725112852892.png)

Host端RNIC将NVMe SQE封装成网络命令包，作为RDMA报文内容（payload），通过RDMA传输到target端，Target端的RNIC解包之后，将SQE和数据放入target端主机的内存中，随后Target处理NVMe命令和数据；处理完之后，将NVMe CQE封装成RDMA响应包(Response)，放入RDMA完成队列中，返回给Host。



## FIO工具介绍

FIO 工具是一款用于测试硬件存储性能的辅助工具，兼具灵活性、可靠性从而从众多性能测试工具中脱颖而出。磁盘的 I/O 是衡量硬件性能的最重要的指标之一，而 FIO 工具通过模拟 I/O负载对存储介质进行压力测试，并将存储介质的 I/O 数据直观的呈现出来。

f关于io介绍的网站：https://fio.readthedocs.io/en/latest/fio_doc.html#how-fio-works

### fio操作

让 fio 模拟所需的 I/O 工作负载的第一步是编写一个描述该特定设置的作业文件(job file)。 作业文件中可以定义要模拟的线程或者文件，通过共享参数定义全局部分，也可以定义每一线程的单独参数。运行时，fio 会解析此文件并按照描述设置所有内容。

### fio文件配置介绍

1. I/O type

   定义发布给文件的I/O模式，读、写、顺序、随机或者是否使用buffer.

   - direct=bool

     ```
     If value is true, use non-buffered I/O. Default: false.
     ```

     这个值决定是否使用non-buffered I/O。

     buffered IO指的是在内核和用户程序之间设置了一层缓冲区，用来提高IO读写的效率；

     读取：硬盘--->内核缓冲区--->**用户缓冲区**--->用户程序

     写回：用户程序--->**用户缓冲区**--->内核缓冲区--->硬盘

   - atomic=bool

     ```
     If value is true, attempt to use atomic direct I/O. Atomic writes are guaranteed to be stable once acknowledged by the operating system. 
     ```

     如果值为 true，则尝试使用原子 I/O。原子I/O的写入是稳定的

   - readwrite=str，rw=str

     ```
     Type of I/O pattern.
     read;write;trim;randread;randwrite;readwrite;randrw;trimwrite
     ```

     

2. Block size

   发出I/O的块大小，可以是单个值，也可以是个范围
   
   - blocksize=int\[,int][,int]
   
     单个值适用于read、write和trim。 可以为read、write和trim指定逗号分隔的值。
   
     ```
     The block size in bytes used for I/O units. Default: 4096. A single value applies to reads, writes, and trims. Comma-separated values may be specified for reads, writes, and trims. A value not terminated in a comma applies to subsequent types.
     ```
   
   - blocksize_range=irange\[,irange][,irange], bsrange=irange\[,irange][,irange]
   
     I/O 单元的一系列块大小（以字节为单位）。 除非设置了 blocksize_unaligned，否则发出的 I/O 单元将始终是最小大小的倍数。**bsrange=1k-4k,2k-8k.**
   
     ```
     A range of block sizes in bytes for I/O units. The issued I/O unit will always be a multiple of the minimum size, unless blocksize_unaligned is set.
     
     Comma-separated ranges may be specified for reads, writes, and trims as described in blocksize.
     ```
   
2. I/O size

   要读写的数据总量
   
   - size=int
   
     此作业的每个线程的文件 I/O 的总大小。 Fio 将一直运行直到传输了这么多字节，除非运行时受到其他选项的限制（例如runtime，或由 io_size 增加/减少）。
   
     
   
2. I/O depth

   如果 I/O 引擎是异步的，希望保持多大的队列深度
   
   - iodepth=int
   
     队列深度，要针对文件保持运行的 I/O 单元数。存储控制器可以处理的未完成I / O请求的数量。即对存储控制器的限制，该存储控制器可以处理I / O请求并将命令发送到磁盘（r / w），并且如果存在超出其处理能力的请求，则丢弃请求。
   
     ```
     Number of I/O units to keep in flight against the file.
     Note that increasing iodepth beyond 1 will not affect synchronous ioengines (except for small degrees when verify_async is in use). Even async engines may impose OS restrictions causing the desired depth not to be achieved. This may happen on Linux when using libaio and not setting direct=1, since buffered I/O is not async on that OS. Keep an eye on the I/O depth distribution in the fio output to verify that the achieved depth is as expected. Default: 1.
     ```
   
   - iodepth_batch_submit=int, iodepth_batch=int
   
     一次提交多少I/O
   
     ```
     This defines how many pieces of I/O to submit at once. It defaults to 1 which means that we submit each I/O as soon as it is available, but can be raised to submit bigger batches of I/O at the time. If it is set to 0 the iodepth value will be used.
     ```
   
     
   
2. Job description

   - numjobs=int
   
     创建此作业的指定数量的克隆。 每个作业克隆都作为一个独立的线程或进程产生。 可用于设置更多执行相同操作的线程/进程。
   
     ```
     Create the specified number of clones of this job. Each clone of job is spawned as an independent thread or process. May be used to setup a larger number of threads/processes doing the same thing. Each thread is reported separately; to see statistics for all clones as a whole, use group_reporting in conjunction with new_group. See --max-jobs. Default: 1.
     ```
   
     ​	
   

## 问题和解决方案

### RDMA

1. 如果host端的一个qp中有多个请求在排队，网卡可以同时发出多个请求吗，还是只能一个一个发？

   

2. 如果target端srq中一个wqe所指向的空间为4k，一个包大小为1k，那么target端来一个包应该先取一个wqe，然后接下来的3个包都用这个wqe？

3. 如果2正确：host端有一个qp，网卡从该qp中同时取了两个命令发出，接收端先后收到了两个来自同一条qp的不同命令，这两个不同命令可以共用同一个wqe吗？

4. QP的一个IO会占用多少WQE，RDMA网卡不解析请求如何获知要多少WQE？

5. SRQ处会出现大IO堵塞小IO的情况

6. 单个RDMA单端操作的数据量是否有上界？如果该操作数据量过大（100G），实际情况中是如何处理的？

   MTU：The maximum size of a packet payload (not including headers) that can be sent / received from a port

   A single request packet will result in multiple read response packets if the read length is greater than the PMTU.

   Inbound RDMA Read Queue Depth (IRD)：The maximum number of incoming outstanding RDMA Read Requests that the RDMA-Capable Controller can handle on a particular RDMA-Capable Protocol Stream at the Data Source. For InfiniBand, the equivalent
   for IRD is the Responder Resources.

7. target端触发RNR之后收到的包应该丢弃还是暂存？如果丢弃的话是后面来的包全部丢弃，还是只丢部分，也就是碰到SRQ空闲时就不丢，只有碰到SRQ满才丢

8. 单个命令，I/O单元和packets的关系

### NVMe

### NVMeoF

1. Target端收到NVMe Read命令，应用轮询NVMe Submission queue后，是否可以同时进行这两种操作：（1）通知SSD准备数据到buffer中  2）下发RDMA Write命令，让网卡从SSD的Buffer中拖数据
2. 
