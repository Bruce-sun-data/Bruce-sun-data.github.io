---
title: 大数据编程第一天
date: 2021-05-11 18:15:16
tag: python spark
typora-root-url: ..
---



因为本科专业是大数据的，但是三年下来好像对大数据没有特别深的了解，所以最近打算跟着B站上的教程学一手大数据的项目。是[黑马程序员_大数据实战之用户画像企业级项目,快速进入500强企业的捷径_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili](https://www.bilibili.com/video/BV1Mp4y1x7y7?t=332)用到了spark的相关知识。

# 什么是用户画像

用户画像通常应用于营销推广和个性化推荐。

1. 数据仓库
   - 离线数据仓库，例如点击流日志数据
   - 实时数据仓库
2. 用户画像
   - 以用户为主题，将用户数据进行标签化，给用户打上标签，获取用户群体和画像
3. 推广营销和个性化推荐
   - 使用用户画像的标签数据

[用户画像：将用户数据标签化，给用户打上标签。]()

> 用户数据有哪些
>
> - 基本信息数据
>   - 基于基本信息数据构建标签，称为`User Personal`
> - 使用APP或网站产生数据
>   - 流量日志数据、订单数据、购物车数据、收藏数据
>   - 构建标签，称为`User Profile`



**重点来了**：经过我几天的观察以及搜索，发现没有办法获取这个教程相关的资料。所以又只能自己一步步学习了。😔，太难了。目前就是一边快速看视频一边努力找适合自己的项目。所以接下来记录的都是我自己的学习过程。

# 搭建大数据环境

第一次知道可以使用ClouderaManager或者Ambari直接对大数据平台进行搭建。本文使用CM安装CDH从而对大数据平台进行安装以及管理。

首先使用VMware搭建了三个Centos系统的主机，并配置了公网ip地址。其中废了好大的劲用来配置，主要参考了下面的文章[ 一、Vmware虚拟机设置固定IP地址_半个西瓜-CSDN博客](https://blog.csdn.net/chsong888/article/details/79358959)

之后根据[cloudera环境搭建_扣脚小生-CSDN博客_cloudera](https://blog.csdn.net/m0_38017084/article/details/82218559?utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-2.control)安装Cloudera环境。



## 集群同步脚本

根据网上的其他资料，补充一下博客中没有的，如何设置集群同步脚本。不过在设置集群同步脚本之前，最好先进行**SSH免密登录**。

这是需要自己完成一个脚本的，从而之后方便调用。

具体代码即运行结果如下

### 编写集群同步自行命令的脚本

```
[root@node101 ~]# vi /usr/local/bin/xcall.sh
[root@node101 ~]# 
[root@node101 ~]# chmod +x /usr/local/bin/xcall.sh
[root@node101 ~]# 
[root@node101 ~]# more `which xcall.sh`
#!/bin/bash
#@author :yinzhengjie
#blog:http://www.cnblogs.com/yinzhengjie
#EMAIL:y1053419035@qq.com


#判断用户是否传参
if [ $# -lt 1 ];then
        echo "请输入参数"
        exit
fi

#获取用户输入的命令
cmd=$@

for (( i=101;i<=103;i++ ))
do
        #使终端变绿色 
        tput setaf 2
        echo ============= node${i}.yinzhengjie.org.cn : $cmd ============
        #使终端变回原来的颜色，即白灰色
        tput setaf 7
        #远程执行命令
        ssh node${i}.yinzhengjie.org.cn  $cmd
        #判断命令是否执行成功
        if [ $? == 0 ];then
                echo "命令执行成功"
        fi
done
[root@node101 ~]# 
[root@node101 ~]# xcall.sh ls -d /home/yinzhengjie/
============= node101.yinzhengjie.org.cn : ls -d /home/yinzhengjie/ ============
/home/yinzhengjie/
命令执行成功
============= node102.yinzhengjie.org.cn : ls -d /home/yinzhengjie/ ============
/home/yinzhengjie/
命令执行成功
============= node103.yinzhengjie.org.cn : ls -d /home/yinzhengjie/ ============
/home/yinzhengjie/
命令执行成功
[root@node101 ~]# 

```

### 编写集群同步配置文件的脚本

这个脚本与之前的脚本的区别就在于脚本中只对后面两个从节点进行了配置。

```
[root@node101 ~]# vi /usr/local/bin/xrsync.sh
[root@node101 ~]# 
[root@node101 ~]# 
[root@node101 ~]# chmod +x /usr/local/bin/xrsync.sh
[root@node101 ~]# 
[root@node101 ~]# 
[root@node101 ~]# more `which xrsync.sh`
#!/bin/bash
#@author :yinzhengjie
#blog:http://www.cnblogs.com/yinzhengjie
#EMAIL:y1053419035@qq.com

#判断用户是否传参
if [ $# -lt 1 ];then
    echo "请输入参数";
    exit
fi


#获取文件路径
file=$@

#获取子路径
filename=`basename $file`

#获取父路径
dirpath=`dirname $file`

#获取完整路径
cd $dirpath
fullpath=`pwd -P`

#同步文件到DataNode
for (( i=102;i<=103;i++ ))
do
    #使终端变绿色 
    tput setaf 2
    echo =========== node${i}.yinzhengjie.org.cn : $file ===========
    #使终端变回原来的颜色，即白灰色
    tput setaf 7
    #远程执行命令
    rsync -lr $filename `whoami`@node${i}.yinzhengjie.org.cn:$fullpath
    #判断命令是否执行成功
    if [ $? == 0 ];then
        echo "命令执行成功"
    fi
done
```

## 配置java环境

三个机子都需要配置java环境。关于如何下载老版本的java链接如下[Java Archive Downloads - Java SE 8 (oracle.com)](https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html)

但是安装的过程中出现了报错

```
java: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory
```

原因应该是在解压`jdk-8u181-linux-x64.tar.gz`的时候，由于jdk文件过大，直接拖入虚拟机的时候造成了文件的损坏，所以解决方案见[(linux解压出错 归档文件中异常的 EOF 文件不完整_liangliang233的博客-CSDN博客](https://blog.csdn.net/liangliang233/article/details/105968513/?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242)之后就可以了。重新安装一下就可以了。

## 安装CM

由于最近CM的官网要求下载相关文件需要购买，所以特此找了一个有百度网盘的下载教程[Centos7离线安装Cloudera Manager 5.14.1 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1185607)但主要的流程还是跟着之前的来的，只是中间换了一些步骤。

主要记录一下rpm的安装方式。

按照[ CDH5.16.1采用rpm+http方式离线部署（图解详细流程）_阿顾的博客-CSDN博客](https://blog.csdn.net/u010452388/article/details/102822956?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242)这篇博客进行的。其中数据库的配置部分其中综合了两篇文章的共同部分。

## 分区扩容

linux的分区如何扩容可以参考这一篇[博客](https://zhuanlan.zhihu.com/p/83340525)。在操作的过程中需要对部分的分区进行扩容。

最后成功搭建了Cloudera-Manager环境，但是由于虚拟机的内存不够，怎么运行都很卡。所以至此，切换了研究的思路。打算自己搭建大数据环境了。

# Spark电商实战项目

这一部分的内容是根据[Spark大型电商项目实战_Erik_ly的博客-CSDN博客](https://blog.csdn.net/u012318074/category_6744423.html)进行的。

## 常用的命令

在这之前先记录常用的命令

> - 启动mysql
>
> ```
> service mysql start
> service mysql status   #验证
> ```
>
> - 启动zookeeper
>
> ```
> zkServer.sh start    #每个节点都要执行
> ```
>
> - 启动hdfs
>
> ```
> start-dfs.sh   #在主机上运行
> jps   #在三台机子上使用jps查看
> ```
>
> - 启动yarn集群
>
> ```
> start-yarn.sh    #hadoop01启动start-yarn.sh
> yarn-daemon.sh start resourceManager   #hadoop02、3手动启动ResourceManager
> ```
>
> - 更改hdfs中的文件的所属用户
>
> ```
> sudo -u hdfs hdfs dfs -chown root /user/root
> ```
>
> - 获取hdfs的正确端口号
>
> ```
> hdfs getconf -confKey fs.default.name
> ```
>
> 

## 在使用过程中常见的问题

这一部分介绍项目在进行和使用的过程中遇到的一些问题。

### 启动hadoop集群的时候没有namenode

在主机的终端使用`start-all.sh`之后，使用jps查看，发现结果如下

```shell
[root@sun sun]# jps
3270 QuorumPeerMain
7159 SecondaryNameNode
2295 Main
7310 ResourceManager
7582 Jps
3215 AlertPublisher
3311 DataNode
```

主要根据这篇文章进行修改[完美解决Hadoop集群无法正常关闭的问题!_Alice菌的博客-CSDN博客](https://blog.csdn.net/weixin_44318830/article/details/104339471)和[hadoop 关闭datanode节点时发生异常：no datanode to stop_分享我的点点滴滴，在成长路上与你同行！-CSDN博客](https://blog.csdn.net/hlx20080808/article/details/56670886)

### hadoop没有办法正常关闭

在进行`stop-all.sh`的时候，显示的是

```shell
[root@master ~]# stop-dfs.sh
Stopping namenodes on [master]
master: no namenode to stop
slave2: no datanode to stop
slave1: no datanode to stop
…
```

这个问题的出现是由于hadoop集群关闭的时候，Namenode或者DataNode的pid文件找不着。所以就需要强制关闭hadoop，即使用`kill -9 pid`.但是在使用这个命令关闭datanode的时候总是会有新的datanode进程被创建，所以需要使用命令`cd /proc/pid`和`cat status`对这个进行的父进程进行查看，kill掉父进程，但是发现一会又创建出来了datanode。

所以需要先使用命令

```shell
find -name dfs
```

，找到所有的dfs文件，之后将除了eclipse后面的那个都删除，之后再删除`/tmp`目录中的一些文件

```shell
[root@sun2 tmp]# rm -rf hsperfdata*
[root@sun2 tmp]# rm -rf hadoop*
[root@sun2 tmp]# rm -rf yarn*
```

之后再使用`jps`进行查看，就发现都没有了。

之后需要修改pid文件的目录，在配置文件中$HADOOP_HOME/conf/hadoop-env.sh中添加如下，

```shell
export HADOOP_PID_DIR=${HADOOP_HOME}/pids
```

### hadoop配置文件

修改master主机修改Hadoop如下配置文件，这些配置文件都位于/usr/local/hadoop/etc/hadoop目录下。
修改slaves：
这里把DataNode的主机名写入该文件，每行一个。这里让master节点主机仅作为NameNode使用。

```shell
sun2
sun3
```

修改core-site.xml，其中`hadoop.tmp.dir`不是一个临时文件，是存放所有hadoop中数据的文件，其目录地址为：`file:/usr/local/hadoop/tmp`。`fs.default.name`

```xml
<configuration>
      <property>
          <name>hadoop.tmp.dir</name>
          <value>file:/usr/local/hadoop/tmp</value>
          <description>Abase for other temporary directories.</description>
      </property>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://master:9000</value>
      </property>
  </configuration>
```

修改hdfs-site.xml

```xml
<configuration>
        <property>
                <name>dfs.name.dir</name>
                <value>/usr/local/hadoop/data/namenode</value>
        </property>
        <property>
                <name>dfs.data.dir</name>
                <value>/usr/local/hadoop/data/datanode</value>
        </property>
        <property>
                <name>dfs.tmp.dir</name>
                <value>/usr/local/hadoop/data/tmp</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>3</value>
        </property>
        <property>
                    <name>dfs.namenode.http.address</name>
                   <value>192.168.220.101:50070</value>
        </property>
</configuration>
```

`dfs.name.dir`这个参数用于确定将HDFS文件系统的**元信息保存**在什么目录下。

`dfs.data.dir`这个参数用于确定将**HDFS文件系统的数据保存**在什么目录下。

## 环境搭建

因为在之前配置过了虚拟机的IP地址，所以这里直接就进行hadoop等环境的配置了。具体细节见原博客，我只说明其中出问题的部分。

> 在原博客中的对path环境变量进行设置的时候都缺少对原始path的引用，一定要注意。

在linux中运行mysql的命令是

```
 mysql -u root -p
```

### hive安装

在hive的安装过程中，配置`hive-site.xml`之后启动hive，出现报错，所以将百度网盘中的`hive-site.xml`复制进虚拟机中 ，进行替换。

> #### Linux中使用vim进行单词查找的操作
>
> 从开头搜索
>
> 在命令模式下，输入/你要查找的字符
>
> 按下回车，可以看到vim把光标移动到该字符处
>
> 再按n（小写）查看下一个匹配
>
> 按N(大写）查看上一个匹配（capslock切换大小写，也可以在小写状态下按shift+n）
>
>  
>
> 从结尾处搜索
>
> ？要搜索的字符串或字符

之后在使用hive创建表格的时候出现了报错

> FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: java.io.IOException Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: "sun/192.168.220.101"; destination host is: "sun":9000; )

这里需要先修改一下之前的`hive-site.xml`文件。

```
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://192.168.220.101:3306/hive_metadata?serverTimezone=GMT</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>
```

数据库连接的部分如上修改。之后在`hive`的`bin`目录下运行

```
./schematool -dbType mysql -initSchema
```

成功之后即可进行下一步。

但是在创建表格的时候继续报错。于是使用命令 `hive -hiveconf hive.root.logger=DEBUG,console\`对hive内的内容进行打印。报错如下

```
ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x
```

这个样子应该是在hadoop中没有给root用户配置权限。定睛一看。发现在hdfs中的user目录下都没有root用户。执行以下命令。

```
sudo -u hdfs hadoop fs -mkdir /user/root     #在hdfs中创建root用户
hadoop fs -chmod 777 /user/root              #给root用户配置权限
```

之后就可以在hive中创建表格了。之后上传`users.txt`文件到`/usr/local`的过程中不知道为什么显示的users表中的内容都是null。但是经过其他的教程来看问题应该不大，所以先不管他。

### zookeeper集群安装

按照文章的步骤进行，没有出现问题。

### kafka安装

按照文章的步骤进行，没有出现问题

### flume安装

在flume的安装过程中出现了问题，其中需要把9000的端口号改成8020

### Spark安装

按照文章的步骤可以进行安装，但是在进行spark作业提交的时候会出现报错，报错如下

> java.lang.OutOfMemoryError: Java heap space
>
> spark处理数据的时候遇到内存不足的报错





### MySQL及客户端安装

这里都是在自己的windows环境中进行配置的，由于这一部分比较占用内存，所以先不进行操作

### 环境搭建-Maven安装及配置idea

现在大部分的开发都是使用idea了，所以这里的开发环境也选择idea了。参考的博客是[使用IntelliJ IDEA 配置Maven（入门）_记事本-CSDN博客_idea配置maven](https://blog.csdn.net/qq_32588349/article/details/51461182)

#### 下载maven级配置

现在的地址在[Index of /dist/maven/maven-3/3.3.9/binaries (apache.org)](https://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/)，







