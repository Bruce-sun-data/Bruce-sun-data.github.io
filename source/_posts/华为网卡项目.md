---
title: 华为网卡项目
date: 2021-12-01 11:30:15
tags: 华为项目
typora-root-url: ..
---


# DPDK和SPDK



## DPDK(Data Plane Development Kit)

简单说，DPDK应用程序运行在操作系统的User Space，利用自身提供的数据面库进行收发包处理，绕过了Linux内核态协议栈，以提升报文处理效率。

DPDK是一组lib库和工具包的集合。最简单的架构描述如下图所示：

![img](https://pic3.zhimg.com/80/v2-f4b703475096e19c669d6cfc7128311e_1440w.jpg)

DPDK由于做用户态polling mode，需要对线程模型、内存管理等系统级的资源做定制的管理。例如dpdk中通过使用cpu的亲和性对thread和core做绑定，单个thread独享一个cpu core。再例如dpdk所使用的大页内存等等。这些基础资源的管理都是为用户态polling mode服务。



## SPDK（Storage Performance Development Kit）

关于SPDK的介绍https://www.cnblogs.com/vlhn/p/7727141.html

SPDK(存储性能开发套件)为**编写用户态存储应用**提供了一套工具和库函数。

SPDK的基石是一个运行在用户空间的、采用轮询模式的、异步的、无锁的NVMe驱动。用户空间应用程序可直接访问SSD盘，而且是零拷贝、高度并行地访问SSD盘。

spdk和dpdk的区别在于，spdk主要针对存储性能的提高，关键是nvme驱动。dpdk主要针对网络性能的提高。核心思想spdk和dpdk大致相同。因此也需要对系统资源做独特的管理，这点和dpdk一样。为了资源复用，spdk在EAL这一层统一采用dpdk的实现。无须再去自己实现一套内存管理、thread模型等底层机制。

# RDMA相关的基础知识

这里有一篇汇总的博客记录RDMA的相关文档和相关知识https://blog.csdn.net/bandaoyu/article/details/112861368

还有这一篇https://blog.csdn.net/t1506376703/article/details/106911631/?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.opensearchhbase&spm=1001.2101.3001.4242.1

## 实现RDMA的技术

目前，有三种支持RDMA的技术：IB(InfiniBand)、RoCE(RDMA over Converged Ethernet)、以太网iWARP(the Internet Wide Area RDMA Protocol)。这三种技术使用同一API，但它们有着不同的物理层和链路层。

IB: 原生支持RDMA，它从硬件级别保证可靠传输；

iWARP: 基于 以太网之上的TCP or SCTP 做 RDMA，利用 TCP or SCTP 达到可靠传输，对网络设备的要求比较少；

RoCE: 基于以太网做 RDMA，消耗的资源比 iWARP 少，支持的特性比 iWARP 多，需要额外做可靠传输。

在以太网解决方案中，RoCE相对于iWARP来说有着明显的优势，这些优势体现在延时、吞吐率和 CPU负载。
<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211207153558432.png" alt="image-20211207153558432" style="zoom:50%;" />

## OFED

Linux Drivers Mellanox OpenFabrics Enterprise Distribution for Linux (MLNX_OFED)是一个单独的VPI(Virtual Protocol Interconnect，虚拟协议互联)软件栈。是网卡的驱动

## infiniband和Ethernet的区别

InfiniBand是一种开放标准的高带宽，低时延，高可靠的[网络互联技术](https://www.zhihu.com/search?q=网络互联技术&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A163104439})。

对于Ethernet毋庸多言，和IP技术构成了目前世界上整个互联网络大厦的基石，所有人与智能设备皆依靠Ethernet实现万物互联，这与其设计初衷就是为了能够实现更好的兼容有关，能让不同的系统可以更好的互连互通，这样使得Ethernet自诞生之初就具有非常强的适应性，经过数十年的发展成为互联网中事实上的标准。

而InfiniBand，作为标准制定的初衷是为了解决高性能计算场景中突破集群中数据传输瓶颈痛点应运而生的一种互连标准，从制定之初就定位为高端应用，互连互通不是主要矛盾，高性能通信才是主要切入点，所以，相对于[Ethernet技术](https://www.zhihu.com/search?q=Ethernet技术&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A163104439})而言，由于定位的差异导致InfiniBand与生俱来就有很多和Ethernet的不同，主要表现在带宽、时延、网络可靠性、和组网方式上。

**InfiniBand就是使用了RDMA技术**。

实验室的网卡是ConnectX® -5 EN Card，是没有办法进行模式选择的，也就是说没有办法在以太网和IB中间进行选择，只可以使用以太网。而在以太网中如果要跑RDMA的话，需要使用RoCE技术。

## RDMA客户端和服务器端交流的几个方式

1. send & recieve

   跟TCP/IP的send/recv是类似的，不同的是RDMA是基于消息的数据传输协议（而不是基于字节流的传输协议），所有数据包的组装都在RDMA硬件上完成的，也就是说OSI模型中的下面4层(传输层，网络层，数据链路层，物理层)都在RDMA硬件上完成。

2. read

   RDMA读操作本质上就是Pull操作, 把[远程系统内存](https://www.zhihu.com/search?q=远程系统内存&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A55142557})里的数据拉回到本地系统的内存里。

3. write

## RoCE

# 基础的对比实验

在RDMA技术中，由于在RDMA中运用了锁争夺的机制，导致在线程增多的情况下，锁机制的延展性会出现问题。即线程增加会导致系统的吞吐量下降。基础的对照实验就是要体现出上述的问题。即跑出来下图的效果

![image-20211202130806588](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211202130806588.png)

论文中对这一块的描述是

![image-20211209191906546](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211209191906546.png)

## 基础流程

1. 安装机器，完成网卡的插入。（√）
2. 找到直连光纤，完成两个机器的直连，线没有问题的话网卡会亮黄灯。（√）
3. 安装驱动，设置网卡的静态ip，完成两个机器的连通。

搭机子，通过交换机进行联通。

![image-20211202130940305](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211202130940305.png)

使用驱动检查网卡的性能和使用情况。查看两台机器的连通性。

按照下图设置实验数据

![image-20211202131719170](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211202131719170.png)

图中cycle的定义如下

![image-20211202131923397](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211202131923397.png)

每个线程发送4KB大小的文件，每个线程和远程机之间创建了1000个连接，每一个cycle代表把一个4kb的数据通过这1000个连接发到远端的主机



## 硬件

在两台机子上进行操作，每台机子的配置如下

![image-20211202092739555](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211202092739555.png)



## 装卡

分别将两个网卡插到对应的两个机子上。然后用光纤和光模块对两个机子的网卡进行连接，网卡会闪黄灯。如果没有闪黄灯的话，一定是光纤或者光模块出现了问题

## 安装驱动

### 环境检测

千万注意：要先进行环境的检测，检测你的环境是否安装了IB卡！

![image-20211204100538405](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211204100538405.png)

### 下载驱动安装包

驱动的下载网站https://blog.csdn.net/ljlfather/article/details/102930454

进入这个网址https://www.mellanox.com/page/products_dyn?product_family=26&mtag=linux_sw_drivers，找到和自己的系统对应的驱动进行下载。比如，我就先下载到了mlnx-en-3.3-1.0.0.0.tgz, 装上后TCP正常工作没问题，RDMA不能用。然后从各种官网的网页里面的各种方法乱试，又找到一个MLNX_OFED_LINUX-3.3-1.0.4.0-ubuntu14.04-x86_64.tgz, 这个才对了。

本次下载的版本如下图所示

![image-20211204131059622](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211204131059622.png)



之后进入驱动的目录进行安装。

```
./mlnxofedinstall --force
```

重新加载驱动

![image-20211204101116532](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211204101116532.png)

之后使用命令`ibstatus`你就可以看到网卡对应的接口的信息了

![image-20211204101209872](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211204101209872.png)

可以发现，这里的link_layer是Ethernet，代表当前的模式是以太网。

在想把模式改成IB的时候，使用命令`mlxconfig -d /dev/mst/mt4119_pciconf0 query`查看网卡的基本信息

![image-20211204102717723](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211204102717723.png)

发现网卡的类型是EN，仅支持Ethernet模式。所以如果要在交换机上使用RDMA的话需要安装RoCE。但是本次实验是两个机子直连的，所以直接使用Ethernet模式就可以了。

## 测试及驱动使用说明

两个机子都安装好驱动以后

一台机子作为服务器端，运行命令

```shell
ib_send_bw -a -c UD -d mlx4_0 -i 1
```

另一台机子作为客户端，运行命令

```shell
ib_send_bw -a -c UD -d mlx4_0 -i 1 172.16.0.102
```

ib_send命令是测试 RDMA 发送处理确定带宽

![image-20211207144656527](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211207144656527.png)

代表测试成功

### 相关命令的使用说明

1. 如果要使用srq，就在命令行的最后加上 **--use-srq**

2. ibstatus：查看网卡的基本信息

   <img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211207150754499.png" alt="image-20211207150754499" style="zoom:80%;" />

3. ib_send_bw：Bandwith and latency test using send transactions.

   ![image-20211209085626412](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211209085626412.png)

   参数说明：

   ```
   ib_send_bw [-p TCP_port][-d device][-i IB_port][-c RC|UC|UD][-m mtu] [-s size][-a][-t tdepth][g][-r rdepth][-n iters][-I size][-b][-V][-e] [-N][-F][IP_address]
   
   where:
   
   TCP_port is the TCP port.(本次实验不怎么用到)
   device is the InfiniBand device.
   IB_port is the InfiniBand port.
   mtu is the size of the MTU.
   size is the size of the messages.
   tdepth is the size of the TX queue.
   rdepth is the size of the RX queue.
   iters is the number of message exchanges.
   IP_address is the IP address of the remote node host
   
   RC:面向连接的可靠服务
   UC:面向连接的不可靠服务
   UD:面向数据报的不可靠服务
   RD：面向非连接（类似UDP）的可靠服务
   面向连接 vs 面向数据报
   相同点：两者的通信均包括双方QP对的参与
   不同点：面向连接的通信若有N个节点与之通信，本机需要N个QP对;面向数据报的通信可以做到N个节点与之通信，本机仅需一个QP队.
   
   ```

   ![image-20211207161755180](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211207161755180.png)

   

   ![image-20211207161827345](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211207161827345.png)

   为了体现多端的拥塞，本次实验多是使用的RC

4. 



## 实验具体配置



#### 卸载驱动

```
/usr/sbin/ofed_uninstall.sh
```



## 配置ip地址

### ubuntu

```
sudo vim /etc/network/interfaces
```

在文件中添加如下内容：

```
auto enp1s0
iface enp1s0 inet static
address 172.16.0.104
netmask 255.255.255.0
broadcast 172.16.0.255
```

enp1s0是网卡名称，通过ifconfig查看，address是要给infiniband网卡配置的ip地址。

重启网络服务：

```
sudo service networking restart
```

### Centos

```
sudo vim /etc/sysconfig/network-scripts/ifcfg-ib0
```

添加如下内容：

```
DEVICE=ib0
BOOTPROTO=static
IPADDR=172.16.0.104
NETMASK=255.255.255.0
BROADCAST=172.16.0.255
NETWORK=172.16.0.0
ONBOOT=yes

```

重启网口：

```
sudo ifdown ib0
sudo ifup ib0
```



## 安装RoCE





## NVMe-oF概述

NVMe-oF全称是：Non-Volatile Memory Express over Fabrics。中文的意思是：基于Fabrics的非易失性存储标准。我将Fabric理解为网络架构，RDMA Fabric就有Infiniband，iWARP，RoCEv1，RoCEv2等多种传输协议。

### NVMe-oF和NVMe的关系

NVMe是什么：NVMe传输是一种抽象协议层，旨在提供可靠的NVMe命令和数据传输，是有关访问非易失性储存（通常是 SSD 磁盘）的接口标准。本质是上建立了多个计算机与存储设备的通路，提高搬运数据的速度。

![image-20211230163244922](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230163244922.png)

当前数据中心中的存储网络架构通常使用的是存储区域网络 (Storage Area Network, SAN) ，SAN 是一种基于块的存储，利用高速架构将服务器与其逻辑磁盘单元 (Logical Disk Unit, LUN) 相连。当前的SAN通常使用采用FC（Fibre Channel）光纤通道或者Ethernet进行远距离传输。其中SAN网络环境中，因采用存储设备类型的不同又可以分为FC-SAN(采用光纤通道存储产品)和IP-SAN(采用ISCSI存储设备)。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230170745869.png" alt="image-20211230170745869" style="zoom:50%;" />

**NVMe-oF的目的就是利用现有成熟的Fabric来跑NVMe，实现NVMe标准在PCIe总线上的扩展也就是把本地高速访问的优势暴露给远端应用。**NVMe-oF可以实现ISCISI的功能，甚至可以更好。在长时间内，NVMe-oF和iSCSI还是长期并存的局面。iSCSI目前已经非常成熟，而NVMe-oF则刚刚开始发展，需要不断地完善，并且借鉴iSCSI协议的一些功能，以支持更多的功能。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230170818645.png" alt="image-20211230170818645" style="zoom:50%;" />

### NVMe-oF

2016年发布的 NVMe-oF 1.0 规范描述了两种Fabric，光纤通道(Fibre Channel)和远程直接内存访问 (RDMA) 。下图是NVME和各种Fabric的关系。可以看到NVMe-oF是在Fabric之上的。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230171628666.png" alt="image-20211230171628666" style="zoom:50%;" />

上图可以更好的展示为，左侧三个灰色框架代表的是NVMe在一个机器内的操作，右边是通过NVMe-oF把NVMe扩展到数据中心中。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230172937194.png" alt="image-20211230172937194" style="zoom:50%;" />

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230193645449.png" alt="image-20211230193645449" style="zoom:67%;" />

NVMe-oF的协议栈如下

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230195604296.png" alt="image-20211230195604296" style="zoom:50%;" />

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230201734587.png" alt="image-20211230201734587" style="zoom:50%;" />

NVMeoF协议栈访问流程

![image-20220725111730264](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20220725111730264.png)

### NVMe Transport Mapping（NVME传输映射）

**NVMe over Fabrics 需要底层 NVMe 传输来提供可靠的 NVMe 命令和数据传输。**

在本地 NVMe 实现中，NVMe 命令和响应通过 PCIe 接口映射到主机中的共享内存。然而，fabric 是建立在发送和接收消息的概念之上的，在端之间没有共享内存。NVMe fabric消息传输旨在通过使用包含一个或多个 NVMe 命令或响应的“capsules”将 NVMe 命令和响应封装到基于消息的系统中。 capsules或capsules与数据的组合独立于特定的fabric，并通过所需fabric发送和接收。就是说**NVMe和Fabric之间没有指定的关系，是松耦合的**。 **NVMe 传输是独立于任何物理互连属性的抽象协议层。**

 在NVMe over Fabrics中，整个 NVMe 多队列模型得到维护，使用普通的 NVMe 提交队列(submission queues)和完成队列(completion queues)，但封装在基于消息的传输上。 NVMe I/O 队列对(submission and completion)是为多核 CPU 设计的，这种低延迟的高效设计在 NVMe over Fabrics 中得到维护。

 下图显示了 NVMe 传输的分类和示例。 NVMe 传输包含内存模型、消息模型或两者的组合。 内存模型是一种通过执行显式内存读写操作在结构节点之间传输命令、响应和数据的模型，而消息模型是一种仅在结构节点之间发送包含命令capsules、响应capsules和数据的消息。 消息/内存模型使用消息和显式内存读写操作的组合在结构节点之间传输命令包、响应包和数据。 数据可以选择性地包含在命令包和响应包中。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230192640428.png" alt="image-20211230192640428" style="zoom:40%;" />

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230185100624.png" alt="image-20211230185100624" style="zoom:50%;" />

```
上面这两个图一个来自OverView（第二个图），一个来自specification(第一个图)，感觉有点冲突。
```

在向 NVMe over Fabrics 设备发送复杂消息时，capsules允许将多条小消息作为一条消息发送，从而提高传输效率并减少延迟。 capsules是一个提交队列条目(submission queue entry)或一个完成队列条目(completion queue entry)，结合了一定数量的数据、元数据或 Scatter-Gather Lists (SGLs)。 元素的内容与本地 NVMe 协议相同，但capsules是一种将它们打包在一起以提高效率的方式。下图是capsules的具体架构。

![image-20211230190840475](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20211230190840475.png)

### NVMe

NVMe是什么：NVMe是一种Host与SSD之间通讯的协议，它在协议栈中隶属高层。

![img](https://img-blog.csdn.net/20180417100139590)

NVMe制定了Host与SSD之间通讯的命令，以及命令如何执行的。

NVMe有两种命令，一种叫Admin Command，用以Host管理和控制SSD；另外一种就是I/O Command，用以Host和SSD之间数据的传输。

NVMe有三种队列：Submission Queue （SQ），Completion Queue（CQ）和Doorbell Register （DB）。 SQ和CQ位于Host的内存中，DB则位于SSD的控制器内部。如下图所示：

![img](https://img-blog.csdn.net/20180417100922685)

SQ位于Host内存中，Host要发送命令时，先把准备好的命令放在SQ中，然后通知SSD来取；CQ也是位于Host内存中，一个命令执行完成，成功或失败，SSD总会往CQ中写入命令完成状态。DB则是在Host发送命令时，不是直接往SSD中发送命令的，而是把命令准备好放在自己的内存中，Host就是通过写SSD端的DB寄存器来告知SSD命令已经处理完毕，可以前来检阅。流程如下图所示：

![img](https://img-blog.csdn.net/20180417101200769)

1. Host写命令到SQ，此时的命令在host的内存中；
2. Host写DB，通知NVMe Controller取命令。通过更新在Controller内部的寄存器SQ Tail Doorbell来完成。
3. Controller收到通知，于是从SQ中取命令。取走命令之后，需要在Controller内部的SQ Head Pointer寄存器中更新Head所在的位置。NVMe没有规定Command存入队列的执行顺序，Controller可以一次取出多个Command进行批量处理。
4. Controller执行命令。执行Read/Wirte Command时，这个过程也会与Host Memory进行数据传递。
5. 命令执行完成，Controller往CQ中写指令执行结果。此时，Controller需要更新内部的CQ Tail Pointer寄存器。
6. Controller发短信通知Host指令完成。Controller通过发送一个中断信息告知Host。
7. 收到短信，Host处理CQ，查看命令完成状态；
8. Host处理完CQ中的命令需要告知controller。Host更新Controller内部的CQ Head Doorbell。

四个寄存器全部放在Controller内存中。也就是说Controller知道这SQ Tail/Head和CQ Tail/Head的全部信息。

而Host仅仅知道自己更新的两个信息SQ Tail和CQ Head。Controller把SQ Head和CQ Tail的信息写入Completion报文中，通知给host。

NVMe 访存路径

![image-20220725111306065](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20220725111306065.png)

#### NVMe Controller Memory Buffer(控制器内存缓冲区)

controller通过将 CAP.CMBS 设置为“1”来指示对 CMB 的支持。 host通过将 CMBMSC.CRE 设置为“1”来表示使用 CMB 的意图。 一旦该位设置为“1”，controller就会通过 CMBLOC 和 CMBSZ 属性指示 CMB 的属性.

CMB 可用于多种用途。 controller通过在 CMBSZ 属性中设置支持标志来指示内存可用于哪些目的。



###  NVMe over ROCE

NVMe over RoCE 调用关系如下图所示，内核 nvme_rdma 模块相当于胶水层，连接 rdma stack 和 nvme core接口，即 NVMe 队列接口可以对接 RDMA 队列接口，进而调用下层 rdma stack 中 verbs 传输接口。

<img src="https://pic4.zhimg.com/v2-adaad1b3a14e2fa115fe0472c8b0955f_r.jpg" alt="preview" style="zoom:50%;" />

#### NVMeoF队列结构

NVMeoF将NVMe队列映射到RDMA队列上。

![image-20220725112852892](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20220725112852892.png)

Host端RNIC将NVMe SQE封装成网络命令包，作为RDMA报文内容（payload），通过RDMA传输到target端，Target端的RNIC解包之后，将SQE和数据放入target端主机的内存中，随后Target处理NVMe命令和数据；处理完之后，将NVMe CQE封装成RDMA响应包(Response)，放入RDMA完成队列中，返回给Host。



## XRC（扩展可靠连接技术）网卡模式

使用SRQ虽然能够大大减少进程的接收缓冲区数目，但是它仍然需要在每个进程之间都建立连接。

在多核系统上， 一个节点内往往运行多个进程，这就会使得单个节点上的连接数目成倍增加，内存开销也会成倍增加。为了解决这个问题，扩展可靠连接(Extended Reliable Connection，简称 XRC)技术被引入到了InfiniBand传输服务中。

<img src="D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20220122194819302.png" alt="image-20220122194819302" style="zoom:50%;" />

允许发送方在发送工作请求里指定远端接收队列。如果使用了SRQ，这意味着消息发送者可以指定消息在达到目的节点后由哪个SRQ来接收，因为SRQ是独立于连接之外能够被多个QP所共享的资源，所以XRC相当于变相地支持了一个本地QP可以向多个远端QP发送消息，也就是说，通过XRC一个进程只需要一条连接或者一个QP即可向远端节点上的所有进程发送消息。

![image-20220122194215743](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20220122194215743.png)

XRC 在请求方(requester)的操作与 RC 类似，在响应方(responder)与 RD 类似。由于这种不对称性，XRC 传输服务中的传输对象有一些独特的特征。

1. XRC INI QP 与常规 RC QP 类似，但没有响应端
2. XRC TGT QP 类似于 RD EEC，但没有请求方
3. XRC SRQ 类似于 RD QP，但没有请求方 

| 名称       | 含义                                                         |
| :--------- | ------------------------------------------------------------ |
| XRC        | eXtended Reliable Connected transport service                |
| XRC INI QP | XRC Initiator QP。这是 XRC 操作的发起者队列。 XRC INI QP 用于发出 XRC 传出请求，并且没有响应(responder)方。 XRC 传入请求将由 XRC TGT QP 处理 |
| XRC TGT QP | XRC Target QP。这是 XRC 操作的响应者。 XRC TGT QP（与 XRC SRQ 一起）用于处理传入的 XRC 请求。 XRC TGT QP 没有请求方。 XRC 传出请求通过 XRC INI QP 发出 |
| XRC SRQ    | 这是为传入的 XRC 请求 post Receive WQE 的Receive Queue。 XRC 请求数据包在扩展标头 (XRCETH) 中携带目标 XRC SRQ 编号，如果需要，将从该编号中获取接收 WQE |
| XRC Domain | 用于关联 XRC TGT QP 和 XRC SRQ 的属性。 XRC 数据包只能以与它们的目的地 XRC TGT QP 相同的 XRC Domain中的 XRC SRQ 为目标 |
| XRCETH     | XRC Extended Transport Header. Present in XRC request packets. |





# 实验的相关命令和具体参数



## 抓包工具

### ibdump

## 性能测试工具

### ib_send_bw

具体内容可以看gitlab网页https://github.com/linux-rdma/perftest

1. 服务器端运行

   ```
   ib_send_bw -a -c UD -d mlx5_0 -i 1 -z
   ```

2. 客户端运行

   ```
   ib_send_bw -a -c UD -d mlx5_0 -i 1 -z 172.16.0.106
   ```

   

重点解释一下`-z`这个参数是什么意思：Communicate with rdma_cm module to exchange data - use regular QPs





# 分流访问NVMe SSD

## SPDK NVMe-oF的介绍

NVMe over Fabrics (NVMe-oF) 协议在 RDMA（iWARP、RoCE、InfiniBand™）、光纤通道和 TCP 等网络结构上扩展了 NVM Express* (NVMe) 块协议的并行性和效率。SPDK 提供用户空间 NVMe-oF 的target和initiator，可通过网络扩展 SPDK 堆栈其余部分的软件效率。SPDK NVMe-oF 的target使用 SPDK user-space, polled-mode NVMe driver 向 NVMe 设备提交和完成 I/O 请求，从而减少软件处理开销。 同样，它将连接固定到 CPU 内核以避免同步和缓存抖动，以便这些连接的数据尽可能靠近 CPU 缓存。

SPDK NVMe-oF target and initiator 使用 Infiniband/RDMA verbs API 来访问支持RDMA的NIC。目前只针对ROCEV2进行了测试。与 SPDK NVMe 驱动程序类似，SPDK 提供了一个user-space, lockless, polled-mode的 NVMe-oF  initiator。host使用initiator建立连接并向 NVMe-oF target内的 NVMe 子系统提交 I/O 请求。NVMe 子系统包含namespace，每个namespace都映射到通过 SPDK 的 bdev 层公开的单个块设备。SPDK 的 bdev 层是块设备抽象层和通用块存储堆栈，在许多操作系统都有类似实现。使用 bdev 接口将存储介质与用于访问存储的前端协议完全解耦。用户可以构建自己的虚拟 bdev，提供复杂的存储服务，并将它们与 SPDK NVMe-oF  target集成，无需额外的代码更改。 NVMe-oF target中可能有许多子系统，每个子系统可能包含许多namespace。子系统和namespace可以通过 JSON-RPC 接口动态配置。



## 常用命令

1. 查看nvme ssd具体有哪几个

   ```
   nvme list
   ```

   可以查看所有连接到当前系统的nvme设备：名称，序列号，大小，LBA 和 serial。

   使用该命令的话可以在host机器上可以查看到target的ssd（前提是target的ssd挂载到了spdk中）

   ```shell
   #通过spdk查看每个硬盘的pcie地址
   ```

   ![image-20220519154020368](D:\Sun1999\MyHexoBlogs\myblogs\source\_posts\images\华为网卡项目\image-20220519154020368.png)

注意spdk驱动和nvme驱动的问题

配置完target在配置host

## 注意事项

如果使用modprobe nvme_rdma出错。大概率问题出现在rdma驱动中



## FIO工具介绍

FIO 工具是一款用于测试硬件存储性能的辅助工具，兼具灵活性、可靠性从而从众多性能测试工具中脱颖而出。磁盘的 I/O 是衡量硬件性能的最重要的指标之一，而 FIO 工具通过模拟 I/O负载对存储介质进行压力测试，并将存储介质的 I/O 数据直观的呈现出来。

f关于io介绍的网站：https://fio.readthedocs.io/en/latest/fio_doc.html#how-fio-works

### fio操作

让 fio 模拟所需的 I/O 工作负载的第一步是编写一个描述该特定设置的作业文件(job file)。 作业文件中可以定义要模拟的线程或者文件，通过共享参数定义全局部分，也可以定义每一线程的单独参数。运行时，fio 会解析此文件并按照描述设置所有内容。

### fio文件配置介绍

1. I/O type

   定义发布给文件的I/O模式，读、写、顺序、随机或者是否使用buffer.

   - direct=bool

     ```
     If value is true, use non-buffered I/O. Default: false.
     ```

     这个值决定是否使用non-buffered I/O。

     buffered IO指的是在内核和用户程序之间设置了一层缓冲区，用来提高IO读写的效率；

     读取：硬盘--->内核缓冲区--->**用户缓冲区**--->用户程序

     写回：用户程序--->**用户缓冲区**--->内核缓冲区--->硬盘

   - atomic=bool

     ```
     If value is true, attempt to use atomic direct I/O. Atomic writes are guaranteed to be stable once acknowledged by the operating system. 
     ```

     如果值为 true，则尝试使用原子 I/O。原子I/O的写入是稳定的

   - readwrite=str，rw=str

     ```
     Type of I/O pattern.
     read;write;trim;randread;randwrite;readwrite;randrw;trimwrite
     ```

     

2. Block size

   发出I/O的块大小，可以是单个值，也可以是个范围
   
   - blocksize=int\[,int][,int]
   
     单个值适用于read、write和trim。 可以为read、write和trim指定逗号分隔的值。
   
     ```
     The block size in bytes used for I/O units. Default: 4096. A single value applies to reads, writes, and trims. Comma-separated values may be specified for reads, writes, and trims. A value not terminated in a comma applies to subsequent types.
     ```
   
   - blocksize_range=irange\[,irange][,irange], bsrange=irange\[,irange][,irange]
   
     I/O 单元的一系列块大小（以字节为单位）。 除非设置了 blocksize_unaligned，否则发出的 I/O 单元将始终是最小大小的倍数。**bsrange=1k-4k,2k-8k.**
   
     ```
     A range of block sizes in bytes for I/O units. The issued I/O unit will always be a multiple of the minimum size, unless blocksize_unaligned is set.
     
     Comma-separated ranges may be specified for reads, writes, and trims as described in blocksize.
     ```
   
2. I/O size

   要读写的数据总量
   
   - size=int
   
2. 

   

